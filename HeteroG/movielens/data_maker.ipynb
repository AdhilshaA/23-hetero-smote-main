{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torch_geometric.datasets import MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MovieLens(root='data', model_name='all-MiniLM-L6-v2')\n",
    "# print(dataset)\n",
    "# data = dataset[0]\n",
    "# torch.save(data, 'data/movlens_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    edge_label=[100836]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# data = torch.load('data/movlens_data.pt')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "# data2= HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2['m'].x = data['movie'].x\n",
    "# # data2['u'].x = data['user'].x\n",
    "# data2['u'].num_nodes = data['user'].num_nodes\n",
    "\n",
    "# data2['m', 'edge', 'u'].edge_index = data['user', 'rates', 'movie'].edge_index\n",
    "# data2['m', 'edge', 'u'].y = data['user', 'rates', 'movie'].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    2,    5,  ..., 9462, 9463, 9503]])\n"
     ]
    }
   ],
   "source": [
    "# print(data2['m', 'edge', 'u'].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4,  ..., 5, 5, 3]) torch.return_types.max(\n",
      "values=tensor(5),\n",
      "indices=tensor(3))\n"
     ]
    }
   ],
   "source": [
    "# print(data2['m', 'edge', 'u'].y, torch.max(data2['m', 'edge', 'u'].y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mm\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1mu\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1m(m, edge, u)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    y=[100836]\n",
      "  },\n",
      "  \u001b[1m(m, walk, m)\u001b[0m={ edge_index=[2, 48620] },\n",
      "  \u001b[1m(m, walk, u)\u001b[0m={ edge_index=[2, 29169] },\n",
      "  \u001b[1m(u, walk, m)\u001b[0m={ edge_index=[2, 3050] },\n",
      "  \u001b[1m(u, walk, u)\u001b[0m={ edge_index=[2, 1830] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# torch.save(data2, 'data/ml_data.pt')\n",
    "data2 = torch.load('data/ml_data.pt')\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the features, labels, and other attributes\n",
    "# x = data['movie'].x  # Node features (if available)\n",
    "# y = data.user  # Node labels (if available)\n",
    "# edge_index = data.edge_index  # Edge indices\n",
    "# edge_attr = data.edge_attr  # Edge attributes (if available)\n",
    "# pos = data.pos  # Node positions (if available)\n",
    "\n",
    "# # Print some information about the data\n",
    "# print(\"Number of nodes:\", data.num_nodes)\n",
    "# print(\"Number of edges:\", data.num_edges)\n",
    "# print(\"Number of features per node:\", data.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict, Counter\n",
    "\n",
    "# neigh_dict = defaultdict(list)\n",
    "# # with open(\"data/neigh_list.txt\", 'r') as file:\n",
    "# #     lines = file.readlines()\n",
    "\n",
    "# edge_list = data2['m', 'edge', 'u'].edge_index\n",
    "\n",
    "# for j in range(edge_list.size(1)): \n",
    "#     neigh_dict[f'u{edge_list[0][j]}'].append(f'm{edge_list[1][j]}')   \n",
    "#     neigh_dict[f'm{edge_list[1][j]}'].append(f'u{edge_list[0][j]}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Nodes: 100%|██████████| 10334/10334 [02:43<00:00, 63.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Random path maker\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "\n",
    "# random_walks = defaultdict(list)\n",
    "\n",
    "# for node in tqdm(neigh_dict.keys(), desc=\"Processing Nodes\"):\n",
    "#     # print(node)\n",
    "#     curNode = node\n",
    "    \n",
    "#     walk_size, u_size, m_size = 0,0,0\n",
    "    \n",
    "#     while walk_size < 100:\n",
    "#         prob =  random.random()\n",
    "#         if prob < 0.5:\n",
    "#             curNode = node\n",
    "#         else:\n",
    "#             if curNode not in list(neigh_dict.keys()):\n",
    "#                 # print(node, curNode)\n",
    "#                 curNode = node\n",
    "                \n",
    "#             # print(neigh_dict[curNode])\n",
    "#             if neigh_dict[curNode] != []:\n",
    "#                 curNode = random.choice(neigh_dict[curNode])\n",
    "#                 if curNode[0] == 'u' and u_size < 50:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     u_size += 1\n",
    "#                 elif curNode[0] == 'm' and m_size < 70:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     m_size += 1\n",
    "#                 walk_size += 1\n",
    "#             else:\n",
    "#                 curNode = node\n",
    "            \n",
    "#             # if walk_size == 157: print(\"poop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/het_neigh_train.txt', 'w') as file:\n",
    "#     for node in list(random_walks.keys()):\n",
    "#         file.write(node + ':' + ','.join(random_walks[node])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# m_edge_list = []\n",
    "# u_edge_list = []\n",
    "# m_m_edge_index = []\n",
    "# m_u_edge_index = []\n",
    "# u_m_edge_index = []\n",
    "# u_u_edge_index = []\n",
    "\n",
    "# with open(\"data/het_neigh_train.txt\", 'r') as file:            \n",
    "#     lines = file.readlines()\n",
    "        \n",
    "# for i, line in enumerate(lines):\n",
    "#     line = line.strip()\n",
    "#     node_type = re.split(':', line)[0][0]\n",
    "#     node_id = int(re.split(':', line)[0][1:])\n",
    "#     neigh_list = re.split(',', re.split(':', line)[1].strip())\n",
    "    \n",
    "#     m_edge_list = [node for node in neigh_list if node.startswith('m')]\n",
    "#     u_edge_list = [node for node in neigh_list if node.startswith('u')]\n",
    "    \n",
    "#     m_counts = Counter(m_edge_list)\n",
    "#     u_counts = Counter(u_edge_list)\n",
    "    \n",
    "#     m_edge_list = [node for node, count in m_counts.most_common(5)]\n",
    "#     u_edge_list = [node for node, count in u_counts.most_common(3)]\n",
    "    \n",
    "#     if node_type == 'm':\n",
    "#         m_m_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in m_edge_list])\n",
    "#         m_u_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in u_edge_list])\n",
    "#     else:\n",
    "#         u_m_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in m_edge_list])\n",
    "#         u_u_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in u_edge_list])\n",
    "\n",
    "# # Concatenate the list of tensors into a single tensor\n",
    "# m_m_edge_index = torch.cat(m_m_edge_index, dim=0).t().contiguous()\n",
    "# m_u_edge_index = torch.cat(m_u_edge_index, dim=0).t().contiguous()\n",
    "# u_m_edge_index = torch.cat(u_m_edge_index, dim=0).t().contiguous()\n",
    "# u_u_edge_index = torch.cat(u_u_edge_index, dim=0).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mm\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1mu\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1m(m, edge, u)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    y=[100836]\n",
      "  },\n",
      "  \u001b[1m(m, walk, m)\u001b[0m={ edge_index=[2, 48620] },\n",
      "  \u001b[1m(m, walk, u)\u001b[0m={ edge_index=[2, 29169] },\n",
      "  \u001b[1m(u, walk, m)\u001b[0m={ edge_index=[2, 3050] },\n",
      "  \u001b[1m(u, walk, u)\u001b[0m={ edge_index=[2, 1830] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# data2['m', 'walk', 'm'].edge_index = m_m_edge_index\n",
    "# data2['m', 'walk', 'u'].edge_index = m_u_edge_index\n",
    "# data2['u', 'walk', 'm'].edge_index = u_m_edge_index\n",
    "# data2['u', 'walk', 'u'].edge_index = u_u_edge_index\n",
    "# print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,  ..., 609, 609, 609],\n",
      "        [562,   0, 599,  ..., 609,  67, 572]])\n"
     ]
    }
   ],
   "source": [
    "# print(data2['u', 'walk', 'u'].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = data2['m','edge','u'].edge_index\n",
    "# for k in range(list.size(1)):\n",
    "#     if k ==3: print(list[0][1355].item())\n",
    "#     if list[0][k].item() == 12:\n",
    "#         print(list[1][k].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Node 'm612' not present in the vocabulary.\n",
      "Warning: Node 'm9114' not present in the vocabulary.\n",
      "Warning: Node 'm2160' not present in the vocabulary.\n",
      "Warning: Node 'm3321' not present in the vocabulary.\n",
      "Warning: Node 'm4014' not present in the vocabulary.\n",
      "Warning: Node 'm7504' not present in the vocabulary.\n",
      "Warning: Node 'm3615' not present in the vocabulary.\n",
      "Warning: Node 'm5654' not present in the vocabulary.\n",
      "Warning: Node 'm5679' not present in the vocabulary.\n",
      "Warning: Node 'm4650' not present in the vocabulary.\n",
      "Warning: Node 'm351' not present in the vocabulary.\n",
      "Warning: Node 'm3998' not present in the vocabulary.\n",
      "Warning: Node 'm7389' not present in the vocabulary.\n",
      "Warning: Node 'm3766' not present in the vocabulary.\n",
      "Warning: Node 'm8272' not present in the vocabulary.\n",
      "Warning: Node 'm874' not present in the vocabulary.\n",
      "Warning: Node 'm4435' not present in the vocabulary.\n",
      "Warning: Node 'm2773' not present in the vocabulary.\n",
      "Warning: Node 'm5585' not present in the vocabulary.\n",
      "Warning: Node 'm3974' not present in the vocabulary.\n",
      "Warning: Node 'm1699' not present in the vocabulary.\n",
      "Warning: Node 'm3534' not present in the vocabulary.\n",
      "Warning: Node 'm2464' not present in the vocabulary.\n",
      "Warning: Node 'm529' not present in the vocabulary.\n",
      "Warning: Node 'm9741' not present in the vocabulary.\n",
      "Warning: Node 'm2790' not present in the vocabulary.\n",
      "Warning: Node 'm1260' not present in the vocabulary.\n",
      "Warning: Node 'm8454' not present in the vocabulary.\n",
      "Warning: Node 'm6280' not present in the vocabulary.\n",
      "Warning: Node 'm7886' not present in the vocabulary.\n",
      "Warning: Node 'm8010' not present in the vocabulary.\n",
      "Warning: Node 'm6940' not present in the vocabulary.\n",
      "Warning: Node 'm9319' not present in the vocabulary.\n",
      "Warning: Node 'm9242' not present in the vocabulary.\n",
      "Warning: Node 'm5325' not present in the vocabulary.\n",
      "Warning: Node 'm9526' not present in the vocabulary.\n",
      "Warning: Node 'm6243' not present in the vocabulary.\n",
      "Warning: Node 'm9052' not present in the vocabulary.\n",
      "Warning: Node 'm6161' not present in the vocabulary.\n",
      "Warning: Node 'm207' not present in the vocabulary.\n",
      "Warning: Node 'm4921' not present in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from itertools import *\n",
    "# dimen = 256\n",
    "# window = 5\n",
    "\n",
    "# def read_random_walk_corpus():\n",
    "#     walks, node_ids = [], []\n",
    "#     with open(\"data/het_neigh_train.txt\", 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         for line in lines:\n",
    "#             parts = line.strip().split(':')\n",
    "#             node_ids.append(parts[0])\n",
    "#             path = parts[1].split(',')\n",
    "#             walks.append(path)\n",
    "\n",
    "#     return walks, node_ids\n",
    "\n",
    "# walk_corpus, node_ids = read_random_walk_corpus()\n",
    "\n",
    "# # Train the Word2Vec model\n",
    "# model = Word2Vec(walk_corpus, vector_size=dimen, window=window, min_count=0, workers=8, sg=1, hs=0, negative=5)\n",
    "\n",
    "# # Save node embeddings to a text file\n",
    "# with open(\"data/node_net_embedding.txt\", 'w') as file:\n",
    "#     for i, node_id in enumerate(node_ids):\n",
    "#         if node_id in model.wv:\n",
    "#             embedding = model.wv[node_id]\n",
    "#             embedding_str = \" \".join(str(val) for val in embedding)\n",
    "#             file.write(node_id + \" \" + embedding_str + \"\\n\")\n",
    "#         else:\n",
    "#             print(f\"Warning: Node '{node_id}' not present in the vocabulary.\")\n",
    "# model.wv.save_word2vec_format(\"data/node_net_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/node_net_embedding.txt\", 'r') as file:            \n",
    "        lines = file.readlines()[1:]       \n",
    "\n",
    "embed_d = 256\n",
    "M_n = 9742\n",
    "U_n = 610\n",
    "m_net_embed = torch.zeros(M_n, embed_d, dtype=torch.float32)\n",
    "u_net_embed = torch.zeros(U_n, embed_d, dtype=torch.float32)\n",
    "        \n",
    "for i, line in enumerate(lines):\n",
    "    entries = line.strip().split()\n",
    "    \n",
    "    if not i: continue\n",
    "    node_type = entries[0][0]\n",
    "    node_id = int(entries[0][1:])\n",
    "    if node_type == 'm':\n",
    "        m_net_embed[node_id] = torch.tensor([float(x) for x in entries[1:]])\n",
    "    elif node_type == 'u':\n",
    "        u_net_embed[node_id] = torch.tensor([float(x) for x in entries[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9742, 256]) torch.Size([610, 256])\n"
     ]
    }
   ],
   "source": [
    "print(m_net_embed.shape, u_net_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for heterogenous neighbour aggregation\n",
    "def aggregate(x, edge_index, num_nodes): \n",
    "    # Separate source and target nodes from the edge index\n",
    "    source_nodes, target_nodes = edge_index[0], edge_index[1]\n",
    "\n",
    "    # Aggregate features for each neighbour using scatter_add\n",
    "    # num_source = torch.max(source_nodes, dim = 0).values.item()\n",
    "    aggr_features = torch.zeros(num_nodes, x.size(1))\n",
    "    aggr_features.index_add_(0, source_nodes, x[target_nodes])\n",
    "\n",
    "    # Normalize the aggregated features\n",
    "    row_sum = torch.bincount(source_nodes, minlength=num_nodes).float().clamp(min=1)\n",
    "    aggr_features /= row_sum.view(-1, 1)\n",
    "\n",
    "    return aggr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_u_list = data2['m','edge','u'].edge_index\n",
    "m_embed = data2['m'].x\n",
    "u_embed = aggregate(m_net_embed, m_u_list, U_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mm\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1mu\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1m(m, edge, u)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    y=[100836]\n",
      "  },\n",
      "  \u001b[1m(m, walk, m)\u001b[0m={ edge_index=[2, 48620] },\n",
      "  \u001b[1m(m, walk, u)\u001b[0m={ edge_index=[2, 29169] },\n",
      "  \u001b[1m(u, walk, m)\u001b[0m={ edge_index=[2, 3050] },\n",
      "  \u001b[1m(u, walk, u)\u001b[0m={ edge_index=[2, 1830] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['m_net_embed'].x = m_net_embed    \n",
    "data2['u_net_embed'].x = u_net_embed\n",
    "data2['u_embed'].x = u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    edge_label=[100836]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('data/movlens_data.pt')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mm\u001b[0m={ x=[9742, 404] },\n",
      "  \u001b[1mu\u001b[0m={ num_nodes=610 },\n",
      "  \u001b[1mm_net_embed\u001b[0m={ x=[9742, 256] },\n",
      "  \u001b[1mu_net_embed\u001b[0m={ x=[610, 256] },\n",
      "  \u001b[1mu_embed\u001b[0m={ x=[610, 256] },\n",
      "  \u001b[1m(m, edge, u)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    y=[100836]\n",
      "  },\n",
      "  \u001b[1m(m, walk, m)\u001b[0m={ edge_index=[2, 48620] },\n",
      "  \u001b[1m(m, walk, u)\u001b[0m={ edge_index=[2, 29169] },\n",
      "  \u001b[1m(u, walk, m)\u001b[0m={ edge_index=[2, 3050] },\n",
      "  \u001b[1m(u, walk, u)\u001b[0m={ edge_index=[2, 1830] },\n",
      "  \u001b[1m(u, edge, m)\u001b[0m={}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = HeteroData()\n",
    "data3['m_embed'].x = data2['m'].x\n",
    "data3['m_net_embed'].x = data2['m_net_embed'].x\n",
    "data3['m_embed'].num_nodes = 9742\n",
    "\n",
    "data3['u_embed'].x = u_embed\n",
    "data3['u_net_embed'].x = data2['u_net_embed'].x\n",
    "data3['u_embed'].num_nodes = 610\n",
    "\n",
    "data3['u', 'walk', 'm'].edge_index  = data2['u', 'walk', 'm'].edge_index \n",
    "data3['u', 'walk', 'u'].edge_index  = data2['u', 'walk', 'u'].edge_index \n",
    "data3['m', 'walk', 'm'].edge_index  = data2['m', 'walk', 'm'].edge_index \n",
    "data3['m', 'walk', 'u'].edge_index  = data2['m', 'walk', 'u'].edge_index \n",
    "\n",
    "data3['u','edge','m'].edge_index = data2['m','edge','u'].edge_index\n",
    "data3['u','edge','m'].y = data2['m','edge','u'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data3, 'data/ml_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mm_embed\u001b[0m={\n",
      "    x=[9742, 404],\n",
      "    num_nodes=9742\n",
      "  },\n",
      "  \u001b[1mm_net_embed\u001b[0m={ x=[9742, 256] },\n",
      "  \u001b[1mu_embed\u001b[0m={\n",
      "    x=[610, 256],\n",
      "    num_nodes=610\n",
      "  },\n",
      "  \u001b[1mu_net_embed\u001b[0m={ x=[610, 256] },\n",
      "  \u001b[1m(u, walk, m)\u001b[0m={ edge_index=[2, 3050] },\n",
      "  \u001b[1m(u, walk, u)\u001b[0m={ edge_index=[2, 1830] },\n",
      "  \u001b[1m(m, walk, m)\u001b[0m={ edge_index=[2, 48620] },\n",
      "  \u001b[1m(m, walk, u)\u001b[0m={ edge_index=[2, 29169] },\n",
      "  \u001b[1m(u, edge, m)\u001b[0m={\n",
      "    edge_index=[2, 100836],\n",
      "    y=[100836]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x: tensor([1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors with requires_grad=True\n",
    "x = torch.tensor([2.2, 3.3, 1.4], requires_grad=True)\n",
    "\n",
    "# Clamp the tensor elements between a minimum and maximum value\n",
    "clamped_tensor = torch.clamp(x, min=0.0, max=2.0)\n",
    "\n",
    "# Use the clamped tensor in further computations\n",
    "output = clamped_tensor.sum()\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "output.backward()\n",
    "\n",
    "# Gradients are now available for the input tensor x\n",
    "print(\"Gradient of x:\", x.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries and directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "data_dir = '/home/deependra/project/23-hetero-smote/HeteroG/data/yelp_kaggle/'\n",
    "\n",
    "business_file = 'yelp_academic_dataset_business.json'\n",
    "review_file = 'yelp_academic_dataset_review.json'\n",
    "user_file = 'yelp_academic_dataset_user.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the subset of yelp dataset for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150346/150346 [00:02<00:00, 57084.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of business data:  150346\n",
      "business data columns:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load business data\n",
    "\n",
    "business_data = []\n",
    "with open(data_dir + business_file, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        business_data.append(list(json.loads(line).values()))\n",
    "    business_header = dict(zip(json.loads(line).keys(), range(len(json.loads(line).keys()))))\n",
    "        \n",
    "print('Total number of business data: ', len(business_data))\n",
    "print('business data columns: ', len(business_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6990280/6990280 [00:43<00:00, 159091.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of review data:  6990280\n",
      "review data columns:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "review_data = []\n",
    "with open(data_dir + review_file, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        review_data.append(list(json.loads(line).values()))\n",
    "    review_header = dict(zip(json.loads(line).keys(), range(len(json.loads(line).keys()))))\n",
    "\n",
    "print('Total number of review data: ', len(review_data))\n",
    "print('review data columns: ', len(review_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987897/1987897 [00:17<00:00, 114726.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of user data:  1987897\n",
      "user data columns:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load user data\n",
    "user_data = []\n",
    "with open(data_dir + user_file, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        user_data.append(list(json.loads(line).values()))\n",
    "    user_header = dict(zip(json.loads(line).keys(), range(len(json.loads(line).keys()))))\n",
    "    \n",
    "print('Total number of user data: ', len(user_data))\n",
    "print('user data columns: ', len(user_header))\n",
    "\n",
    "del lines, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business header: \n",
      "{'business_id': 0, 'name': 1, 'address': 2, 'city': 3, 'state': 4, 'postal_code': 5, 'latitude': 6, 'longitude': 7, 'stars': 8, 'review_count': 9, 'is_open': 10, 'attributes': 11, 'categories': 12, 'hours': 13}\n",
      "\n",
      "user header: \n",
      "{'user_id': 0, 'name': 1, 'review_count': 2, 'yelping_since': 3, 'useful': 4, 'funny': 5, 'cool': 6, 'elite': 7, 'friends': 8, 'fans': 9, 'average_stars': 10, 'compliment_hot': 11, 'compliment_more': 12, 'compliment_profile': 13, 'compliment_cute': 14, 'compliment_list': 15, 'compliment_note': 16, 'compliment_plain': 17, 'compliment_cool': 18, 'compliment_funny': 19, 'compliment_writer': 20, 'compliment_photos': 21}\n",
      "\n",
      "review header: \n",
      "{'review_id': 0, 'user_id': 1, 'business_id': 2, 'stars': 3, 'useful': 4, 'funny': 5, 'cool': 6, 'text': 7, 'date': 8}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'business header: \\n{business_header}\\n')\n",
    "print(f'user header: \\n{user_header}\\n')\n",
    "print(f'review header: \\n{review_header}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking and adding businesses, reviews, and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Restaurants', 52268), ('Food', 27781), ('Shopping', 24395), ('Home Services', 14356), ('Beauty & Spas', 14292), ('Nightlife', 12281), ('Health & Medical', 11890), ('Local Services', 11198), ('Bars', 11065), ('Automotive', 10773), ('Event Planning & Services', 9895), ('Sandwiches', 8366), ('American (Traditional)', 8139), ('Active Life', 7687), ('Pizza', 7093), ('Coffee & Tea', 6703), ('Fast Food', 6472), ('Breakfast & Brunch', 6239), ('American (New)', 6097), ('Hotels & Travel', 5857), ('Home & Garden', 5799), ('Fashion', 5739), ('Burgers', 5636), ('Arts & Entertainment', 5434), ('Auto Repair', 5433), ('Hair Salons', 5046), ('Nail Salons', 4621), ('Mexican', 4600), ('Italian', 4573), ('Specialty Food', 4233), ('Doctors', 3763), ('Pets', 3758), ('Real Estate', 3577), ('Seafood', 3539), ('Fitness & Instruction', 3293), ('Professional Services', 3270), ('Hair Removal', 3239), ('Desserts', 3186), ('Chinese', 3169), ('Bakeries', 3150), ('Grocery', 3139), ('Salad', 3064), ('Hotels', 2977), ('Chicken Wings', 2966), ('Cafes', 2756), ('Ice Cream & Frozen Yogurt', 2657), ('Caterers', 2645), ('Pet Services', 2626), ('Dentists', 2528), ('Skin Care', 2498), ('Venues & Event Spaces', 2480), ('Tires', 2471), ('Beer', 2413), ('Wine & Spirits', 2413), ('Delis', 2393), ('Oil Change Stations', 2356), ('Waxing', 2333), ('Contractors', 2327), (\"Women's Clothing\", 2184), ('Massage', 2134), ('Sports Bars', 2110), ('Day Spas', 1997), ('General Dentistry', 1993), ('Education', 1936), ('Flowers & Gifts', 1930), ('Auto Parts & Supplies', 1926), ('Apartments', 1921), ('Convenience Stores', 1919), ('Home Decor', 1892), ('Gyms', 1831), ('Japanese', 1830), ('Pubs', 1821), ('Cocktail Bars', 1718), ('Sushi Bars', 1717), ('Barbeque', 1694), ('Juice Bars & Smoothies', 1684), ('Barbers', 1677), ('Car Dealers', 1665), ('Sporting Goods', 1662), ('Accessories', 1639), ('Cosmetic Dentists', 1630), ('Drugstores', 1630), ('Local Flavor', 1604), ('Furniture Stores', 1597), ('Pet Groomers', 1549), ('Asian Fusion', 1547), ('Cosmetics & Beauty Supply', 1544), ('Jewelry', 1525), ('Steakhouses', 1506), ('Diners', 1494), ('Financial Services', 1487), ('Trainers', 1460), ('Hair Stylists', 1459), ('Arts & Crafts', 1440), ('Department Stores', 1430), ('Electronics', 1422), ('Veterinarians', 1383), ('Massage Therapy', 1357), ('Pet Sitting', 1356), ('Eyelash Service', 1278)]\n"
     ]
    }
   ],
   "source": [
    "b_classes = []\n",
    "for i in range(len(business_data)):\n",
    "    if business_data[i][business_header['categories']] != None:\n",
    "        b_classes.extend(business_data[i][business_header['categories']].split(', '))\n",
    "\n",
    "print(Counter(b_classes).most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150346/150346 [00:00<00:00, 406624.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hotel businesses: 2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = ['Hotels']\n",
    "business_ids = []\n",
    "for i in tqdm(range(len(business_data))):\n",
    "    if business_data[i][business_header['categories']]:\n",
    "        category = business_data[i][business_header['categories']].split(', ')\n",
    "        category = list(set(category).intersection(categories))\n",
    "        if len(category) == 1: \n",
    "            business_ids.append(business_data[i][business_header[\"business_id\"]])\n",
    "\n",
    "business_ids = dict(zip(business_ids, range(len(business_ids))))\n",
    "print(f'Number of hotel businesses: {len(business_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all review ids and user ids...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6990280 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6990280/6990280 [00:03<00:00, 2175508.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 126018\n",
      "review counts: 1stars: 45400, 2stars: 0, 3stars: 24954, 4stars: 0, 5stars: 55664\n",
      "Current Number of users: 109573\n",
      "previous number of businesses: 2977\n",
      "Current number of businesses: 2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "review_ids = []\n",
    "user_ids = []\n",
    "business_ids2 = []\n",
    "stars = [0,0, 0, 0, 0] # stars 1 to 5 counts\n",
    "print('Collecting all review ids and user ids...')\n",
    "for i in tqdm(range(len(review_data))):\n",
    "    if review_data[i][review_header['business_id']] in business_ids.keys() and review_data[i][review_header['text']] and review_data[i][review_header['stars']] in [1,3,5]:\n",
    "        review_ids.append(review_data[i][review_header['review_id']])\n",
    "        user_ids.append(review_data[i][review_header['user_id']])\n",
    "        business_ids2.append(review_data[i][review_header['business_id']])\n",
    "        stars[int(review_data[i][review_header['stars']]) - 1] += 1\n",
    "\n",
    "# setting up review ids\n",
    "print(f'Number of reviews: {len(review_ids)}')\n",
    "print(f'review counts: 1stars: {stars[0]}, 2stars: {stars[1]}, 3stars: {stars[2]}, 4stars: {stars[3]}, 5stars: {stars[4]}')\n",
    "\n",
    "# setting up user ids\n",
    "user_ids = list(set(user_ids))\n",
    "print(f'Current Number of users: {len(user_ids)}')\n",
    "\n",
    "print(f'previous number of businesses: {len(business_ids)}')\n",
    "business_ids = list(set(business_ids2))\n",
    "print(f'Current number of businesses: {len(business_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all user ids...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987897/1987897 [00:01<00:00, 1850623.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for unuseable user ids...\n",
      "Number of bad user ids: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6990280/6990280 [00:01<00:00, 3657599.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad review ids (same as bad user ids): 0\n",
      "Final Number of users: 109573\n",
      "Final Number of reviews: 126018\n",
      "Final Number of businesses: 2974\n"
     ]
    }
   ],
   "source": [
    "# checking for unuseable user ids\n",
    "\n",
    "all_user_ids = []\n",
    "print('Collecting all user ids...')\n",
    "for i in tqdm(range(len(user_data))):\n",
    "    all_user_ids.append(user_data[i][user_header['user_id']])\n",
    "    \n",
    "print('Checking for unuseable user ids...')\n",
    "bad_user_ids = set(user_ids) - set(all_user_ids)\n",
    "print(f'Number of bad user ids: {len(bad_user_ids)}')\n",
    " \n",
    "bad_review_ids = [] # from bad user ids\n",
    "\n",
    "for i in tqdm(range(len(review_data))):\n",
    "    if review_data[i][review_header['user_id']] in bad_user_ids and review_data[i][review_header['review_id']] in review_ids:\n",
    "        bad_review_ids.append(review_data[i][review_header['review_id']])\n",
    "        \n",
    "print(f'Number of bad review ids (same as bad user ids): {len(bad_review_ids)}')\n",
    "\n",
    "# removing bad user ids\n",
    "user_ids = list(set(user_ids) - bad_user_ids)\n",
    "\n",
    "# removing bad review ids\n",
    "review_ids = list(set(review_ids) - set(bad_review_ids))\n",
    "\n",
    "user_ids = dict(zip(user_ids, range(len(user_ids)))) # nodes\n",
    "review_ids = dict(zip(review_ids, range(len(review_ids)))) # edges\n",
    "business_ids = dict(zip(business_ids, range(len(business_ids)))) #nodes\n",
    "\n",
    "print(f'Final Number of users: {len(user_ids)}')\n",
    "print(f'Final Number of reviews: {len(review_ids)}')\n",
    "print(f'Final Number of businesses: {len(business_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the business_id to review_id and them to user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6990280/6990280 [00:03<00:00, 2151836.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user-business edges: 126018\n",
      "Number of review samples with classes: 126018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ub_edges = []\n",
    "\n",
    "user_id_index = review_header['user_id']\n",
    "business_id_index = review_header['business_id']\n",
    "review_id_index = review_header['review_id']\n",
    "\n",
    "r_classes = []\n",
    "for i in tqdm(range(len(review_data))):\n",
    "    if review_data[i][review_id_index] in review_ids:\n",
    "        ub_edges.append([user_ids[review_data[i][user_id_index]], business_ids[review_data[i][business_id_index]]])\n",
    "        r_classes.append([review_ids[review_data[i][review_id_index]], int((review_data[i][review_header['stars']]) // 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the business_id, review_id, and user_id to txt files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving business ids...\n",
      "Saving user ids...\n",
      "Saving review ids...\n",
      "Saving user review edges...\n",
      "Saving review business edges...\n",
      "Saving business classes...\n"
     ]
    }
   ],
   "source": [
    "print('Saving business ids...')\n",
    "with open(data_dir + 'business_ids.txt', 'w') as f:\n",
    "    for business,id in business_ids.items():\n",
    "        f.write(f'{id}\\t{business}\\n')\n",
    "print('Saving user ids...')\n",
    "with open(data_dir + 'user_ids.txt', 'w') as f:\n",
    "    for user,id in user_ids.items():\n",
    "        f.write(f'{id}\\t{user}\\n')\n",
    "print('Saving review ids...')\n",
    "with open(data_dir + 'review_ids.txt', 'w') as f:\n",
    "    for review,id in review_ids.items():\n",
    "        f.write(f'{id}\\t{review}\\n')\n",
    "print('Saving user  business edges...')\n",
    "with open(data_dir + 'ub_edges.txt', 'w') as f:\n",
    "    for edge in ub_edges:\n",
    "        f.write(f'{edge[0]}\\t{edge[1]}\\n')\n",
    "print('Saving review edge classes...')\n",
    "with open(data_dir + 'review_edgeclasses.txt', 'w') as f:\n",
    "    for edge in b_classes:\n",
    "        f.write(f'{edge[0]}\\t{edge[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random walk with restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = True\n",
    "if load_data:\n",
    "    business_ids = {}\n",
    "    user_ids = {}\n",
    "    review_ids = {}\n",
    "    ub_edges = []\n",
    "    r_class = []\n",
    "    \n",
    "    with open(data_dir + 'business_ids.txt', 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            business_ids[line.strip().split('\\t')[1]] = int(line.strip().split('\\t')[0])\n",
    "    \n",
    "    with open(data_dir + 'user_ids.txt', 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            user_ids[line.strip().split('\\t')[1]] = int(line.strip().split('\\t')[0])\n",
    "    \n",
    "    with open(data_dir + 'review_ids.txt', 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            review_ids[line.strip().split('\\t')[1]] = int(line.strip().split('\\t')[0])\n",
    "    \n",
    "    with open(data_dir + 'ub_edges.txt', 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            ub_edges.append([int(line.strip().split('\\t')[0]), int(line.strip().split('\\t')[1])])\n",
    "            \n",
    "    with open(data_dir + 'review_edgeclasses.txt', 'r') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            r_class.append([int(line.strip().split('\\t')[0]), int(line.strip().split('\\t')[1])])\n",
    "            \n",
    "    print(f'TOtal number of business ids: {len(business_ids)}')\n",
    "    print(f'TOtal number of user ids: {len(user_ids)}')\n",
    "    print(f'TOtal number of review ids: {len(review_ids)}')\n",
    "    print(f'TOtal number of ub edges: {len(ub_edges)}')\n",
    "    print(f'TOtal number of r samples with classes: {len(r_class)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all neighbors of each node\n",
    "\n",
    "print('Creating all neighbors dict for each node...')\n",
    "all_neighbors = {f'b{id}':[] for name,id in business_ids.items()}\n",
    "all_neighbors.update({f'u{id}':[] for name,id in user_ids.items()})\n",
    "print('nodes created: ', len(all_neighbors))\n",
    "print('Finding all neighbors of each node...')\n",
    "for i in tqdm(range(len(ub_edges))):\n",
    "    all_neighbors[f'u{ub_edges[i][0]}'].append(f'b{ub_edges[i][1]}')\n",
    "    all_neighbors[f'b{ub_edges[i][1]}'].append(f'r{ub_edges[i][0]}')\n",
    "    \n",
    "print('Total number of nodes: ', len(all_neighbors))\n",
    "\n",
    "count = 0\n",
    "for node,neighbors in all_neighbors.items():\n",
    "    if not neighbors:\n",
    "        count += 1\n",
    "print(f'Number of nodes with no neighbors: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: random walk with a restart (HAN paper method)\n",
    "\n",
    "length = 100\n",
    "prob_restart = 0.5\n",
    "max_samples = {'b' : 25, 'u': 75} # should add up to length\n",
    "\n",
    "random_walks = {}\n",
    "for node in tqdm(list(all_neighbors.keys())):\n",
    "    random_walks[node] = []\n",
    "    curr_node = node\n",
    "    neighbors = 0\n",
    "    neigh_b = 0\n",
    "    neigh_u = 0\n",
    "    while neighbors < length:\n",
    "        # print(f'finding {neighbors}th neighbor of {node}')\n",
    "        p = random.random()\n",
    "        if p < prob_restart:\n",
    "            curr_node = node\n",
    "        else:\n",
    "            curr_node = random.choice(all_neighbors[curr_node])\n",
    "            # if curr_node == node:\n",
    "            #     curr_node = random.choice(all_neighbors[curr_node])\n",
    "            if curr_node[0] == 'b' and neigh_b < max_samples['b']:\n",
    "                random_walks[node].append(curr_node)\n",
    "                neigh_b += 1\n",
    "                neighbors += 1\n",
    "            elif curr_node[0] == 'u' and neigh_u < max_samples['u']:\n",
    "                random_walks[node].append(curr_node)\n",
    "                neigh_u += 1\n",
    "                neighbors += 1\n",
    "\n",
    "print(f\"number of random walks: {len(random_walks)}\")\n",
    "print(f\"singular random walk, 'u0': {random_walks['u0']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating random walks file\n",
    "with open(data_dir + 'random_walks.txt', 'w') as f:\n",
    "    for key, value in tqdm(random_walks.items()):\n",
    "        f.write(f\"{key}:{','.join(value)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping and finding top neighbors of each node type for all node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: Grouping different types of neighbors based on frequency(HAN paper method)\n",
    "\n",
    "top_k = {'b' : 5, 'u' : 15} # top k neighbors to be considered for each type of node\n",
    "                                     # preferred to be less than the sample size in random walk for the optional last step\n",
    "\n",
    "top_neighbors = {}\n",
    "for node in tqdm(list(all_neighbors.keys())):\n",
    "    \n",
    "    # initializing top_neighbors categores for that node\n",
    "    top_neighbors[node] = {'b' : [], 'u' : []}\n",
    "    \n",
    "    # finding neighbors of different types\n",
    "    neigh_b = []\n",
    "    neigh_u = []\n",
    "    for neigh in random_walks[node]:\n",
    "        if neigh[0] == 'b':\n",
    "            neigh_b.append(neigh)\n",
    "        elif neigh[0] == 'u':\n",
    "            neigh_u.append(neigh)\n",
    "    \n",
    "    # finding top k neighbors (and their countes)\n",
    "    top_b = Counter(neigh_b).most_common(top_k['b'])\n",
    "    top_u = Counter(neigh_u).most_common(top_k['u'])\n",
    "    \n",
    "    # adding top k neighbors to top_neighbors in nodes respective category\n",
    "    top_neighbors[node]['b'].extend([i[0] for i in top_b])\n",
    "    top_neighbors[node]['u'].extend([i[0] for i in top_u])\n",
    "\n",
    "\n",
    "    # adding random neighbors if less than top k\n",
    "    if len(top_b) < top_k['b']:\n",
    "        top_neighbors[node]['b'].extend(random.sample(neigh_b,top_k['b'] - len(top_b)))\n",
    "    if len(top_u) < top_k['u']:\n",
    "        top_neighbors[node]['u'].extend(random.sample(neigh_u,top_k['u'] - len(top_u)))\n",
    "        \n",
    "del neigh_b, neigh_u, top_b, top_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating top neighbors file\n",
    "with open(data_dir + 'node_neighbors_top.txt', 'w') as f:\n",
    "    for key, value in tqdm(top_neighbors.items()):\n",
    "        f.write(f\"{key}:{','.join(value['b'])};{','.join(value['u'])};{','.join(value['r'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG HeteroData object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOtal number of business ids: 52244\n",
    "# TOtal number of user ids: 1243707\n",
    "# TOtal number of review ids: 3189729\n",
    "# TOtal number of ur edges: 3189729\n",
    "# TOtal number of rb edges: 3189729\n",
    "# TOtal number of r samples with classes: 3189729\n",
    "\n",
    "\n",
    "Nb = 52244 # business nodes\n",
    "Nu = 1243707 # user nodes\n",
    "Nr = 3189729 # review edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfmidc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import random\n",
    "\n",
    "data_dir = \"/home/deependra/project/GraphSMOTE/HeteroG/data/academic/\"\n",
    "\n",
    "\n",
    "a_trainfile = \"a_class_train.txt\"\n",
    "\n",
    "ap_trainfile = \"a_p_list_train.txt\"\n",
    "pa_trainfile = \"p_a_list_train.txt\"\n",
    "\n",
    "pp_file = \"p_p_citation_list.txt\"\n",
    "vp_trainfile = \"v_p_list_train.txt\"\n",
    "\n",
    "nodes_embed_file = \"node_net_embedding.txt\"\n",
    "p_titlefile = \"p_title_embed.txt\"\n",
    "p_abstractfile = \"p_abstract_embed.txt\"\n",
    "\n",
    "# test files and others\n",
    "ap_testfile = \"a_p_list_test.txt\"\n",
    "pa_testfile = \"p_a_list_test.txt\"\n",
    "pp_testfile = \"p_p_cite_list_test.txt\" \n",
    "# vp missing\n",
    "a_testfile = \"a_class_test.txt\"\n",
    "\n",
    "pp_trainfile = \"p_p_cite_list_train.txt\" #another pp citation file was used in train by paper\n",
    "pv_file = \"p_v.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_a = 28646\n",
    "N_p = 21044\n",
    "N_v = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of node embeddings: 38772 (including the shape, extra line and a erroneous line)\n",
      "length of a sinel node embedding: 129 (including name of node) \n",
      "\n",
      "author, paper, venue counts in node embedding: [22821, 15930, 18]\n",
      "\n",
      " Single node sample embedding\n",
      "['v17', '-0.5037959', '0.5946551', '0.31017154', '-0.19049475', '0.12888494', '0.050235868', '-0.81903493', '-0.15262532', '-0.32429516', '0.06192694', '0.15981801', '-0.03078058', '0.26008233', '-0.30948904', '-0.050463047', '-0.3913203', '-0.15056635', '-0.13715981', '0.090905525', '0.20138781', '-0.16433519', '-0.017016286', '-0.072366305', '0.4541567', '-0.8047796', '0.24664292', '-0.251392', '-0.028795026', '-0.38743636', '-0.049152117', '-0.17575642', '0.13272819', '-0.03008081', '0.028684866', '-0.583873', '-0.16135879', '-0.08571402', '0.31543538', '-0.35522765', '-0.10304684', '-0.3879957', '-0.05874115', '0.084208265', '1.011872', '-0.08729001', '0.041401695', '0.5747423', '-0.29516315', '0.35220018', '0.12039722', '0.4176981', '0.42023966', '0.4588378', '-0.4079293', '0.03936286', '-0.1286031', '0.32869896', '-0.012008862', '-0.11764544', '-0.25943395', '0.017396964', '0.09727604', '-0.03558714', '0.50860465', '0.04864586', '0.39651904', '-0.6985504', '-0.24825686', '-0.61408556', '-0.1446922', '-0.5622648', '0.12489937', '-0.010816564', '-0.097676806', '0.118643574', '-0.37230372', '0.3689778', '-0.3917894', '-0.1786293', '0.05827119', '0.39899847', '0.25222376', '-0.15347256', '-0.11139945', '-0.2798669', '0.43773186', '-0.46402147', '0.03073877', '-0.073680684', '-0.18700463', '0.2213496', '-0.004424464', '-0.08746136', '-0.26975897', '-0.28196135', '-0.028274873', '0.9512285', '-0.55860627', '0.24058838', '0.084069766', '-0.084362455', '-0.0019059808', '-0.026070384', '-0.20576504', '-0.25089657', '-0.056340624', '0.2847583', '0.19523747', '-1.0332723', '0.54266846', '0.37312627', '-0.066507965', '0.2222583', '-0.13729669', '0.2669997', '-0.0778851', '-0.18025729', '-0.29859155', '0.097786546', '-0.006633747', '-0.21644321', '0.22879295', '0.05044401', '-0.002125007', '0.122717924', '0.26869848', '0.3220822', '-0.29652107\\n']\n"
     ]
    }
   ],
   "source": [
    "#not needed for data loading\n",
    "\n",
    "\n",
    "with open(data_dir + nodes_embed_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"number of node embeddings: {len(lines)} (including the shape, extra line and a erroneous line)\")\n",
    "# print(lines[3])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"length of a sinel node embedding: {len(lines[2].split(' '))} (including name of node) \")\n",
    "\n",
    "apv_counts= [0, 0, 0]\n",
    "for line in lines:\n",
    "    if 'a' in line.split(' ')[0]:\n",
    "        apv_counts[0] += 1\n",
    "    elif 'p' in line.split(' ')[0]:\n",
    "        apv_counts[1] += 1\n",
    "    elif 'v' in line.split(' ')[0]:\n",
    "        apv_counts[2] += 1\n",
    "print(f\"\\nauthor, paper, venue counts in node embedding: {apv_counts}\")\n",
    "\n",
    "print(\"\\n Single node sample embedding\")\n",
    "print(lines[3].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 1125 (train author count)\n",
      "\n",
      " Single train a_class sample:\n",
      "['62', '0\\n']\n",
      "\n",
      "\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not needed for data loading\n",
    "\n",
    "with open(data_dir + a_trainfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (train author count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\n Single train a_class sample:\")\n",
    "print(lines[3].split(','))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "ac_train = [[line.split(',')[0], line.split(',')[1]] for line in lines]\n",
    "print(len(ac_train[0]))\n",
    "ac_train = np.array(ac_train, dtype=np.float32)\n",
    "ac_train  = torch.from_numpy(ac_train)\n",
    "ac_train[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 20171 (ap train author count)\n",
      "\n",
      " Single sample:\n",
      "['4', '9287\\n']\n",
      "total train ap edges: 42379\n",
      "total unique papers in train ap edges: 13250\n",
      "total unique authors in train ap edges: 20171\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + ap_trainfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (ap train author count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\n Single sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "ap_list = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        ap_list.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total train ap edges: {len(ap_list)}\")\n",
    "print(f\"total unique papers in train ap edges: {len(set([x[1] for x in ap_list]))}\")\n",
    "print(f\"total unique authors in train ap edges: {len(set([x[0] for x in ap_list]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 and ['a', 'b', 'c']\n",
      "2 and ['d', 'e', 'f']\n",
      "3 and ['g', 'h', 'i']\n",
      "[[1, 'a'], [1, 'b'], [1, 'c'], [2, 'd'], [2, 'e'], [2, 'f'], [3, 'g'], [3, 'h'], [3, 'i']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['1', 'a'],\n",
       "       ['1', 'b'],\n",
       "       ['1', 'c'],\n",
       "       ['2', 'd'],\n",
       "       ['2', 'e'],\n",
       "       ['2', 'f'],\n",
       "       ['3', 'g'],\n",
       "       ['3', 'h'],\n",
       "       ['3', 'i']], dtype='<U21')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = ['1:a,b,c', '2:d,e,f', '3:g,h,i']\n",
    "for line in lines:\n",
    "    print(line.split(':')[0],\"and\" ,line.split(':')[1].split(','))\n",
    "\n",
    "edges = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        edges.append([int(line.split(':')[0]), neigh])\n",
    "print(edges)\n",
    "edges = np.array(edges)\n",
    "edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 13250 (pa paper count)\n",
      "\n",
      " Single sample:\n",
      "['3', '8423,11932,2850,12137,6483\\n']\n",
      "total pa edges: 42379\n",
      "total unique papers in pa edges: 13250\n",
      "total unique authors in pa edges: 20171\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + pa_trainfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (pa paper count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\n Single sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pa_list = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pa_list.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total pa edges: {len(pa_list)}\")\n",
    "print(f\"total unique papers in pa edges: {len(set([x[0] for x in pa_list]))}\")\n",
    "print(f\"total unique authors in pa edges: {len(set([x[1] for x in pa_list]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 5945 (paper count)\n",
      "\n",
      "Single sample:\n",
      "['97', '8643,9167\\n']\n",
      "total pp edges: 21357\n",
      "unique papers in pp edges: 9104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(data_dir + pp_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines in file: {len(lines)} (paper count)\")\n",
    "# print(lines[0])\n",
    "\n",
    "print(\"\\nSingle sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pp_list = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pp_list.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total pp edges: {len(pp_list)}\")\n",
    "print(f\"unique papers in pp edges: {len(set([x[0] for x in pp_list] + [x[1] for x in pp_list]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 18 (venue count)\n",
      "\n",
      "Single sample:\n",
      "['3', '2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,7267,7268,7269,7270,7271,7272,7273,7274,7275,7276,7277,7278,7279,7280,7281,7282,7283,7284,7285,7286,7287,7288,7289,7290,7291,7292,7293,7294,7295,7296,7297,7298,7299,7300,7301,7302,7303,7304,7305,7306,7307,7308,7309,7310,7311,7312,7313,7314,7315,7316,7317,7318,7319,7320,7321,7322,7323,8616,8617,8618,8619,8620,8621,8622,8623,8624,8625,8626,8627,8628,8629,8630,8631,8632,8633,8634,8635,8636\\n']\n",
      "total vp edges: 13250\n",
      "unique venues in vp edges: 18\n",
      "unique papers in vp edges: 13250\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + vp_trainfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines in file: {len(lines)} (venue count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\nSingle sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "vp_list = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        vp_list.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total vp edges: {len(vp_list)}\")\n",
    "print(f\"unique venues in vp edges: {len(set([x[0] for x in vp_list]))}\")\n",
    "print(f\"unique papers in vp edges: {len(set([x[1] for x in vp_list]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.1970e-03, -1.0716e-02,  2.1190e-03, -2.5350e-03,  4.6950e-03,\n",
      "        -7.2370e-03,  9.5600e-04, -7.1500e-04,  5.5900e-03, -6.3200e-04,\n",
      "         1.0150e-03,  6.6610e-03,  4.5680e-03, -1.7378e-02,  3.9060e-03,\n",
      "        -1.6425e-02, -1.4570e-03, -2.6110e-03, -4.3500e-03,  7.9320e-03,\n",
      "         1.5803e-02,  2.8610e-03,  6.4330e-03,  7.8380e-03,  4.7880e-03,\n",
      "         1.3100e-04, -2.9160e-03, -9.7510e-03, -1.5930e-03, -5.3200e-04,\n",
      "        -2.2540e-03, -2.6320e-03, -8.0770e-03, -1.9710e-03, -6.3050e-03,\n",
      "        -1.0737e-02,  1.7230e-02, -3.6460e-03,  1.4900e-04,  2.4500e-04,\n",
      "        -6.6090e-03,  6.3230e-03,  1.7405e-02,  5.2230e-03, -6.0850e-03,\n",
      "         1.2507e-02,  9.0370e-03, -1.1490e-02, -5.7990e-03, -5.2710e-03,\n",
      "        -7.4030e-03,  7.8030e-03,  1.0564e-02,  1.1130e-02, -8.3400e-04,\n",
      "        -1.2722e-02,  4.9120e-03, -4.9990e-03, -9.8380e-03,  2.7180e-03,\n",
      "         2.5540e-03, -1.2735e-02, -3.5880e-03, -2.0450e-03, -4.4440e-03,\n",
      "         1.0704e-02, -1.0094e-02, -2.5740e-03, -2.5270e-03, -4.5970e-03,\n",
      "         1.0371e-02,  2.6660e-03,  7.2200e-03, -1.5765e-02,  1.9804e-02,\n",
      "        -7.7190e-03,  1.8048e-02,  1.1083e-02, -2.3840e-03, -9.3730e-03,\n",
      "         1.6410e-03, -6.5410e-03, -4.9720e-03, -9.1820e-03,  2.8490e-03,\n",
      "         1.4703e-02,  7.9670e-03,  5.0500e-04, -3.5160e-03,  6.7510e-03,\n",
      "         1.5387e-02,  2.4950e-03, -3.2400e-04,  5.8290e-03, -9.5930e-03,\n",
      "         1.6029e-02, -2.1300e-04, -1.7956e-02,  6.3070e-03,  6.7390e-03,\n",
      "        -1.7427e-02, -3.6100e-04, -1.7300e-03, -2.3510e-03,  3.4610e-03,\n",
      "        -3.9430e-03,  1.1170e-02,  1.5140e-02,  2.7700e-04, -7.8900e-04,\n",
      "         1.8350e-03,  7.6610e-03, -4.7270e-03, -6.1940e-03, -2.1520e-03,\n",
      "         7.7860e-03, -3.0620e-03,  1.0949e-02, -5.2530e-03,  1.9860e-03,\n",
      "         5.6200e-04, -3.1510e-03,  9.8560e-03, -1.1507e-02, -3.7980e-03,\n",
      "         7.8280e-03,  5.9100e-04,  5.4560e-03,  2.1303e-01, -2.4532e-01,\n",
      "        -2.4636e-02,  2.8721e-01,  1.2451e-01, -2.4539e-01,  6.2183e-02,\n",
      "         3.0477e-01,  8.4815e-02, -1.6024e-01,  1.9720e-01,  2.7741e-01,\n",
      "        -1.7581e-01, -2.5642e-01,  7.2462e-02, -1.4055e-01, -8.3285e-02,\n",
      "        -5.1647e-02, -9.8283e-02, -7.9870e-02,  1.0793e-01, -1.9085e-01,\n",
      "         1.2060e-01, -1.4956e-02,  2.0716e-01, -5.5015e-02,  2.8142e-01,\n",
      "        -2.8582e-01, -1.8956e-01, -3.6722e-02,  2.7731e-02, -1.0940e-03,\n",
      "         4.8969e-02,  1.0515e-01, -1.9098e-01, -1.0624e-01,  1.2865e-01,\n",
      "         9.3572e-02,  1.4800e-02, -5.8058e-02, -1.2430e-01,  1.8751e-01,\n",
      "         7.3177e-02, -3.8559e-02, -9.4647e-02, -9.2966e-02, -8.6931e-02,\n",
      "        -7.3103e-02,  8.8280e-02,  1.5665e-01,  1.0009e-01, -1.1343e-01,\n",
      "        -3.6646e-02,  1.5981e-01,  1.8996e-01, -9.0678e-02,  1.2004e-01,\n",
      "        -1.2668e-01, -6.5631e-02, -9.6663e-02,  4.5162e-02, -2.8877e-01,\n",
      "         7.7605e-02,  8.1282e-02,  2.2448e-02,  8.2251e-02, -4.7380e-03,\n",
      "        -1.7049e-02, -9.1765e-02, -1.2965e-01, -4.3508e-02,  2.4083e-01,\n",
      "        -1.2337e-02, -2.5170e-02,  8.2340e-03, -9.8210e-02,  5.0008e-02,\n",
      "        -8.4126e-02, -1.0036e-01, -9.8846e-02,  6.5947e-02, -1.9317e-01,\n",
      "        -5.4067e-02, -2.5464e-01, -6.2486e-02,  2.1849e-02, -2.2013e-01,\n",
      "         2.6316e-01,  6.1748e-02,  1.5864e-01,  1.6564e-01, -1.7872e-01,\n",
      "         1.3327e-01,  7.9611e-02,  1.7656e-01,  2.9736e-02, -1.6550e-01,\n",
      "        -2.2598e-01, -8.1150e-03,  2.8340e-02,  9.2900e-04, -1.5490e-02,\n",
      "         6.9280e-03, -1.0839e-01,  4.3654e-02, -2.8997e-02,  3.6483e-01,\n",
      "        -1.2873e-01, -1.4319e-01, -1.2109e-02,  1.5601e-01, -4.7060e-03,\n",
      "        -6.1520e-02, -4.1311e-02,  1.5996e-01, -9.3947e-02,  8.3863e-02,\n",
      "         1.6723e-01,  1.0766e-01,  3.6623e-02, -7.8432e-02,  1.4626e-01,\n",
      "        -2.1437e-02, -1.0057e-01, -6.3651e-02,  2.4480e-03, -1.6854e-01,\n",
      "         1.2978e-01])\n",
      "total papers: 21044\n",
      "total paper features: 256\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + p_titlefile, 'r') as f:\n",
    "    titles = f.readlines()\n",
    "with open(data_dir + p_abstractfile, 'r') as f:\n",
    "    abstracts = f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "paper_x = []\n",
    "for i in range(1,len(titles)):\n",
    "    paper_x.append(titles[i].strip().split(' ')[1:] + abstracts[i].strip().split(' ')[1:])\n",
    "paper_x = np.array(paper_x, dtype=np.float32)\n",
    "paper_x = torch.from_numpy(paper_x)\n",
    "\n",
    "print(paper_x[0])\n",
    "print(f\"total papers: {len(paper_x)}\")\n",
    "print(f\"total paper features: {len(paper_x[0])}\")\n",
    "# print([i for i in range(len(paper_x)) if len(paper_x[i]) != 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTHORS\n",
      "------\n",
      "total authors in training labels: 1125\n",
      "total authors in ap edges: 20171\n",
      "total authors in both: 1125\n",
      "total authors in either: 20171 (all training class labels are from training authors)\n",
      "\n",
      "PAPERS\n",
      "------\n",
      "total unique papers in all paper related edges: 14845\n",
      "\n",
      "unique papers in ap edges: 13250\n",
      "unique papers in vp edges: 13250 (these edges in ap and vp are same)\n",
      "\n",
      "unique papers in pp edges: 9104\n",
      "number of citing papers without author and venue: 1514\n",
      "number of cited papers without author and venue: 220\n",
      "number of extra papers connected without author and venue in pp edges:  1595\n"
     ]
    }
   ],
   "source": [
    "# total paper, venue and author analysis\n",
    "authors_classtrain = set([x[0].item() for x in author_y])\n",
    "authors_ap_train = set([x[0] for x in ap_list])\n",
    "print(f\"AUTHORS\\n------\\ntotal authors in training labels: {len(authors_classtrain)}\")\n",
    "print(f\"total authors in ap edges: {len(authors_ap_train)}\")\n",
    "print(f\"total authors in both: {len(authors_classtrain.intersection(authors_ap_train))}\")\n",
    "print(f\"total authors in either: {len(authors_classtrain.union(authors_ap_train))} (all training class labels are from training authors)\\n\")\n",
    "\n",
    "\n",
    "paperlist = set([x[1] for x in ap_list] + [x[1] for x in vp_list] + [x[0] for x in pp_list] + [x[1] for x in pp_list])\n",
    "\n",
    "print(f\"PAPERS\\n------\\ntotal unique papers in all paper related edges: {len(paperlist)}\\n\")\n",
    "print(f\"unique papers in ap edges: {len(set([x[1] for x in ap_list]))}\")\n",
    "print(f\"unique papers in vp edges: {len(set([x[1] for x in vp_list]))} (these edges in ap and vp are same)\")\n",
    "\n",
    "print(f\"\\nunique papers in pp edges: {len(set([x[0] for x in pp_list] + [x[1] for x in pp_list]))}\")\n",
    "extra_papers = [] # those papers only been cited not have author or venue edges\n",
    "common_papers = list(set([x[1] for x in ap_list] + [x[1] for x in vp_list])) #having author and venue edges\n",
    "# print(len(set([x[1] for x in ap_list] + [x[1] for x in vp_list])))\n",
    "\n",
    "count = 0  \n",
    "for paper in set([x[0] for x in pp_list]):\n",
    "    if paper not in common_papers:\n",
    "        count += 1\n",
    "print(f\"number of citing papers without author and venue: {count}\")\n",
    "count = 0\n",
    "for paper in set([x[1] for x in pp_list]):\n",
    "    if paper not in common_papers:\n",
    "        count += 1\n",
    "print(f\"number of cited papers without author and venue: {count}\")\n",
    "\n",
    "count = 0\n",
    "for paper in set([x[0] for x in pp_list] + [x[1] for x in pp_list]):\n",
    "    if paper not in common_papers:\n",
    "        count += 1\n",
    "        extra_papers.append(paper)\n",
    "print(\"number of extra papers connected without author and venue in pp edges: \", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test files    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 10235 (test author-class pairs count)\n",
      "\n",
      " Single test a_class sample:\n",
      "['5', '0\\n']\n",
      "total authors in test labels: 10235\n",
      "total authors in both test_labels and trained authors: 10235\n",
      " (means that all test labels are also involved in training)\n",
      "(thus training can be done with the current data setting, wheere train labels are used to train the model and test labels are used to test it just for accuracy)\n",
      "(This means that no test graph was used, similar strategy was in graphsmote therefore acceptable)\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + a_testfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (test author-class pairs count)\")\n",
    "# print(lines[0])\n",
    "\n",
    "print(\"\\n Single test a_class sample:\")\n",
    "print(lines[3].split(','))\n",
    "ac_test = [[int(line.split(',')[0]), int(line.split(',')[1])] for line in lines]\n",
    "authors_classtest = set([int(x.split(',')[0]) for x in lines])\n",
    "print(f\"total authors in test labels: {len(authors_classtest)}\")\n",
    "print(f\"total authors in both test_labels and trained authors: {len(authors_classtest.intersection(authors_ap_train))}\\n (means that all test labels are also involved in training)\\n(thus training can be done with the current data setting, wheere train labels are used to train the model and test labels are used to test it just for accuracy)\")\n",
    "print(f\"(This means that no test graph was used, similar strategy was in graphsmote therefore acceptable)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 13745 (ap test author count)\n",
      "\n",
      " Single sample:\n",
      "['59', '10746\\n']\n",
      "total test ap edges: 26932\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + ap_testfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (ap test author count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\n Single sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pa_trainlist = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pa_trainlist.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total test ap edges: {len(pa_trainlist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines in file: 7794 (pa test author count)\n",
      "\n",
      " Single sample:\n",
      "['8872', '4309\\n']\n",
      "total test pa edges: 26932\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + pa_testfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"total lines in file: {len(lines)} (pa test author count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\n Single sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pa_testlist = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pa_testlist.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total test pa edges: {len(pa_testlist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 4431 (pp_edges paper train count)\n",
      "\n",
      "Single sample:\n",
      "['97', '8643,9167\\n']\n",
      "total train pp edges: 14583\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + pp_trainfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines in file: {len(lines)} (pp_edges paper train count)\")\n",
    "# print(lines[0])\n",
    "\n",
    "print(\"\\nSingle sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pp_trainlist = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pp_trainlist.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total train pp edges: {len(pp_trainlist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 3920 (pp_edges paper test count)\n",
      "\n",
      "Single sample:\n",
      "['8875', '649\\n']\n",
      "total test pp edges: 19655\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + pp_testfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines in file: {len(lines)} (pp_edges paper test count)\")\n",
    "# print(lines[0])\n",
    "\n",
    "print(\"\\nSingle sample:\")\n",
    "print(lines[3].split(':'))\n",
    "\n",
    "pp_testlist = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(':')[1].split(','):\n",
    "        pp_testlist.append([int(line.split(':')[0]), int(neigh)])\n",
    "print(f\"total test pp edges: {len(pp_testlist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if we used p_p_cite_train.txt\n",
      "unique papers in pp train edges: 6850\n",
      "papers common in pp_trainedges and train_papers with authors: 6850\n",
      "\n",
      " if we used p_p_cite_test.txt\n",
      "unique papers in pp test edges: 8886\n",
      "unique papers in ap_test edges:  7794\n",
      "papers common in pp_testedges and test_papers with authors: 4111 (vp test file missing)\n",
      "papers total in ap_test and pp_test: 12369\n",
      " \n",
      "papers involved in both train and test pp edges:  3542\n",
      "papers citing in both train and test: 0 (means papers citing are exclusive but paper being cited are not exclusive)\n",
      " (also means that the pp edges in train and test are exclusive since directed edges)\n"
     ]
    }
   ],
   "source": [
    "print(f\" if we used p_p_cite_train.txt\")\n",
    "print(f\"unique papers in pp train edges: {len(set([x[0] for x in pp_trainlist] + [x[1] for x in pp_trainlist]))}\")\n",
    "print(f\"papers common in pp_trainedges and train_papers with authors: {len(set([x[0] for x in pp_trainlist] + [x[1] for x in pp_trainlist]).intersection(common_papers))}\\n\") \n",
    "\n",
    "print(f\" if we used p_p_cite_test.txt\")\n",
    "print(f\"unique papers in pp test edges: {len(set([x[0] for x in pp_testlist] + [x[1] for x in pp_testlist]))}\")\n",
    "print(\"unique papers in ap_test edges: \", len(set([x[0] for x in pa_testlist])))\n",
    "print(f\"papers common in pp_testedges and test_papers with authors: {len(set([x[0] for x in pp_testlist] + [x[1] for x in pp_testlist]).intersection(set([x[1] for x in pa_testlist])))} (vp test file missing)\") \n",
    "print(f\"papers total in ap_test and pp_test: {len(set([x[0] for x in pa_testlist] + [x[0] for x in pp_testlist] + [x[1] for x in pp_testlist]))}\\n \")\n",
    "print(\"papers involved in both train and test pp edges: \", len(set([x[0] for x in pp_trainlist] + [x[1] for x in pp_trainlist]).intersection(set([x[0] for x in pp_testlist] + [x[1] for x in pp_testlist]))))\n",
    "print(f\"papers citing in both train and test: {len(set([x[0] for x in pp_trainlist]).intersection(set([x[0] for x in pp_testlist])))} (means papers citing are exclusive but paper being cited are not exclusive)\\n (also means that the pp edges in train and test are exclusive since directed edges)\")\n",
    "# print(f\"papers being cited in both train and test: {len(set([x[1] for x in pp_trainlist]).intersection(set([x[1] for x in pp_testlist])))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 21044 (venue count)\n",
      "\n",
      "Single sample:\n",
      "['3', '1\\n']\n",
      "total vp edges: 21044\n",
      "unique venues in vp edges: 18\n",
      "unique papers in vp edges: 21044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(data_dir + \"p_v.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(f\"Total lines in file: {len(lines)} (venue count)\")\n",
    "# print(lines[-1])\n",
    "\n",
    "print(\"\\nSingle sample:\")\n",
    "print(lines[3].split(','))\n",
    "\n",
    "pv_fulllist = []\n",
    "for line in lines:\n",
    "    for neigh in line.split(',')[1].split(','):\n",
    "        pv_fulllist.append([int(line.split(',')[0]), int(neigh)])\n",
    "print(f\"total vp edges: {len(pv_fulllist)}\")\n",
    "print(f\"unique venues in vp edges: {len(set([x[1] for x in pv_fulllist]))}\")\n",
    "print(f\"unique papers in vp edges: {len(set([x[0] for x in pv_fulllist]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data as PyG.HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    num_nodes=28646,\n",
      "    y=[1125],\n",
      "    y_index=[1125]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={\n",
      "    num_nodes=21044,\n",
      "    x=[21044, 256]\n",
      "  },\n",
      "  \u001b[1mvenue\u001b[0m={ num_nodes=18 },\n",
      "  \u001b[1m(paper, cites, paper)\u001b[0m={ edge_index=[2, 21357] },\n",
      "  \u001b[1m(author, writes, paper)\u001b[0m={ edge_index=[2, 42379] },\n",
      "  \u001b[1m(paper, is_written_by, author)\u001b[0m={ edge_index=[2, 42379] },\n",
      "  \u001b[1m(venue, publishes, paper)\u001b[0m={ edge_index=[2, 13250] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "full_data = HeteroData()\n",
    "\n",
    "\n",
    "full_data['author'].num_nodes = 28646\n",
    "full_data['paper'].num_nodes = 21044\n",
    "full_data['venue'].num_nodes = 18\n",
    "\n",
    "full_data['paper'].x = paper_x\n",
    "full_data['author'].y = ac_train[:,1].long()\n",
    "full_data['author'].y_index = ac_train[:, 0].long()\n",
    "\n",
    "full_data['paper', 'cites', 'paper'].edge_index = torch.Tensor(pp_list).T.long()\n",
    "full_data['author', 'writes', 'paper'].edge_index = torch.Tensor(ap_list).T.long()\n",
    "full_data['paper', 'is_written_by', 'author'].edge_index = torch.Tensor(pa_list).T.long()\n",
    "full_data['venue', 'publishes', 'paper'].edge_index = torch.Tensor(vp_list).T.long()\n",
    "\n",
    "print(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Loading imbalanced data as PyG.HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total classes in train: 4; [0, 1, 2, 3]\n",
      "total classes in test: 4; [0, 1, 2, 3]\n",
      "\n",
      "TRAIN CLASSES\n",
      "class 0: 335 authors\n",
      "class 1: 202 authors\n",
      "class 2: 299 authors\n",
      "class 3: 289 authors\n",
      "min authors in a class: 202\n"
     ]
    }
   ],
   "source": [
    "trainclasses = list(set([int(x[1].item()) for x in ac_train]))\n",
    "print(f\"total classes in train: {len(trainclasses)}; {trainclasses}\")\n",
    "\n",
    "testclasses = list(set([x[1] for x in ac_test]))\n",
    "print(f\"total classes in test: {len(testclasses)}; {testclasses}\")\n",
    "\n",
    "print(f\"\\nTRAIN CLASSES\")\n",
    "class_authors = {cls: [] for cls in trainclasses}\n",
    "for ac in ac_train:\n",
    "    class_authors[int(ac[1].item())].append(int(ac[0].item()))\n",
    "\n",
    "for cls in trainclasses:\n",
    "    print(f\"class {cls}: {len(class_authors[cls])} authors\")\n",
    "min_authors = min([len(class_authors[cls]) for cls in trainclasses])\n",
    "print(f\"min authors in a class: {min_authors}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of authors in each class (imbalanced): [200, 200, 200, 140]\n"
     ]
    }
   ],
   "source": [
    "# DATA imbalance settings\n",
    "n_full = 200 # take only less than 202\n",
    "ratio_classes = [1,1,1,0.7]\n",
    "random_seed_value = 42\n",
    "random.seed(random_seed_value)\n",
    "\n",
    "\n",
    "#setting imbalance to classes\n",
    "n_eachclass = [int(n_full*cls) for cls in ratio_classes]\n",
    "print(f\"number of authors in each class (imbalanced): {n_eachclass}\")\n",
    "\n",
    "imbalanced_a_idx = []\n",
    "for cls in trainclasses:\n",
    "    imbalanced_a_idx.append(random.sample(class_authors[cls], n_eachclass[cls]))\n",
    "# print(f\"imbalanced class 3: {imbalanced_a_idx[3]}\")\n",
    "\n",
    "ac_train_imbalanced = []\n",
    "for ac in ac_train:\n",
    "    if int(ac[0].item()) in imbalanced_a_idx[int(ac[1].item())]:\n",
    "        ac_train_imbalanced.append([ac[0].item(), ac[1].item()])\n",
    "ac_train_imbalanced = np.array(ac_train_imbalanced, dtype=np.float32)\n",
    "ac_train_imbalanced  = torch.from_numpy(ac_train_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    num_nodes=28646,\n",
      "    y=[740],\n",
      "    y_index=[740]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={\n",
      "    num_nodes=21044,\n",
      "    x=[21044, 256]\n",
      "  },\n",
      "  \u001b[1mvenue\u001b[0m={ num_nodes=18 },\n",
      "  \u001b[1m(paper, cites, paper)\u001b[0m={ edge_index=[2, 21357] },\n",
      "  \u001b[1m(author, writes, paper)\u001b[0m={ edge_index=[2, 42379] },\n",
      "  \u001b[1m(paper, is_written_by, author)\u001b[0m={ edge_index=[2, 42379] },\n",
      "  \u001b[1m(venue, publishes, paper)\u001b[0m={ edge_index=[2, 13250] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# loading imbalanced HEteroData\n",
    "\n",
    "imbalanced_data = HeteroData()\n",
    "\n",
    "\n",
    "imbalanced_data['author'].num_nodes = 28646\n",
    "imbalanced_data['paper'].num_nodes = 21044\n",
    "imbalanced_data['venue'].num_nodes = 18\n",
    "\n",
    "imbalanced_data['paper'].x = paper_x\n",
    "imbalanced_data['author'].y = ac_train_imbalanced[:,1].long()\n",
    "imbalanced_data['author'].y_index = ac_train_imbalanced[:, 0].long()\n",
    "\n",
    "imbalanced_data['paper', 'cites', 'paper'].edge_index = torch.Tensor(pp_list).T.long()\n",
    "imbalanced_data['author', 'writes', 'paper'].edge_index = torch.Tensor(ap_list).T.long()\n",
    "imbalanced_data['paper', 'is_written_by', 'author'].edge_index = torch.Tensor(pa_list).T.long()\n",
    "imbalanced_data['venue', 'publishes', 'paper'].edge_index = torch.Tensor(vp_list).T.long()\n",
    "\n",
    "print(imbalanced_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfmidc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

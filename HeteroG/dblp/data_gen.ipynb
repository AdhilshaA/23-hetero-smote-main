{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import tqdm\n",
    "# from torch_geometric.datasets import DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      "), None)\n"
     ]
    }
   ],
   "source": [
    "# data = torch.load('data/processed/data.pt')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([2, 2, 3,  ..., 0, 0, 0])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[-0.6924, -0.4659,  1.1540,  ...,  0.9178,  0.1995, -0.6360],\n",
      "        [ 1.2031, -0.4003,  0.0740,  ...,  1.3262, -0.3325,  0.8198],\n",
      "        [ 0.3748,  0.5731,  0.4802,  ...,  1.1522,  0.6010, -0.4309],\n",
      "        ...,\n",
      "        [ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
      "        [ 0.1724, -0.2723, -1.3368,  ..., -0.0881,  0.0225,  0.1166],\n",
      "        [ 0.2197,  0.0253,  0.1220,  ...,  0.0871, -0.5351, -0.4949]])\n",
      "20\n",
      "tensor([[    0,     0,     1,  ...,  4054,  4055,  4056],\n",
      "        [ 2364,  6457,  2365,  ..., 13891, 13891, 13892]])\n",
      "tensor([[    0,     1,     2,  ..., 14327, 14327, 14327],\n",
      "        [  262,   263,   263,  ...,   324,  1068,  3647]])\n",
      "tensor([[    0,     0,     0,  ..., 14327, 14327, 14327],\n",
      "        [    4,     5,     6,  ...,   586,   730,  1311]])\n",
      "tensor([[   0,    0,    0,  ..., 7720, 7721, 7722],\n",
      "        [  19,   30,  225,  ..., 5166, 5168, 5174]])\n",
      "tensor([[    0,     1,     2,  ..., 14325, 14326, 14327],\n",
      "        [    0,     0,     0,  ...,    19,    19,    19]])\n",
      "tensor([[    0,     0,     0,  ...,    19,    19,    19],\n",
      "        [    0,     1,     2,  ..., 14325, 14326, 14327]])\n"
     ]
    }
   ],
   "source": [
    "# a_data = data[0]['author'].x\n",
    "# a_label = data[0]['author'].y\n",
    "# p_data = data[0]['paper'].x\n",
    "# t_data = data[0]['term'].x\n",
    "\n",
    "# c_num = data[0]['conference'].num_nodes\n",
    "# ap_edges = data[0]['author','to','paper'].edge_index\n",
    "# pa_edges = data[0]['paper','to','author'].edge_index\n",
    "# pt_edges = data[0]['paper','to','term'].edge_index\n",
    "# tp_edges = data[0]['term','to','paper'].edge_index\n",
    "# pc_edges = data[0]['paper','to','conference'].edge_index\n",
    "# cp_edges = data[0]['conference','to','paper'].edge_index\n",
    "\n",
    "# print(a_data)\n",
    "# print(a_label)\n",
    "# print(p_data)\n",
    "# print(t_data)\n",
    "# print(c_num)\n",
    "# print(ap_edges)\n",
    "# print(pa_edges)\n",
    "# print(pt_edges)\n",
    "# print(tp_edges)\n",
    "# print(pc_edges)\n",
    "# print(cp_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbour reader for all nodes\n",
    "# neigh_dict = defaultdict(list)\n",
    "# edge_lists = [ap_edges, tp_edges, cp_edges, pa_edges, pt_edges,  pc_edges]\n",
    "\n",
    "# for i, edge_list in enumerate(edge_lists):\n",
    "    \n",
    "#     for col in range(edge_list.size(1)):\n",
    "#         if i == 0:\n",
    "#             neigh_dict[f'a{edge_list[0][col]}'].append(f'p{edge_list[1][col]}')\n",
    "#         elif i == 1:\n",
    "#             neigh_dict[f't{edge_list[0][col]}'].append(f'p{edge_list[1][col]}')\n",
    "#         elif i == 2:\n",
    "#             neigh_dict[f'c{edge_list[0][col]}'].append(f'p{edge_list[1][col]}')\n",
    "#         else: \n",
    "#             if i == 3: n = 'a'\n",
    "#             elif i == 4: n = 't'\n",
    "#             else: n = 'c'\n",
    "#             neigh_dict[f'p{edge_list[0][col]}'].append(f'{n}{edge_list[1][col]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(neigh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random path maker\n",
    "# random_walks = defaultdict(list)\n",
    "\n",
    "# for node in list(neigh_dict.keys()):\n",
    "#     # print(node)\n",
    "#     curNode = node\n",
    "    \n",
    "#     walk_size, a_size, p_size, t_size, c_size = 0,0,0,0,0\n",
    "    \n",
    "#     while walk_size < 160:\n",
    "#         prob =  random.random()\n",
    "#         if prob < 0.5:\n",
    "#             curNode = node\n",
    "#         else:\n",
    "#             # if curNode not in list(neigh_dict.keys()):\n",
    "#             #     print(node, curNode)\n",
    "#             # print(neigh_dict[curNode])\n",
    "#             curNode = random.choice(neigh_dict[curNode])\n",
    "#             if curNode != node:\n",
    "#                 if curNode[0] == 'a' and a_size < 60:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     a_size += 1\n",
    "#                 elif curNode[0] == 'p' and p_size < 90:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     p_size += 1\n",
    "#                 elif curNode[0] == 't' and t_size < 60:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     t_size += 1\n",
    "#                 elif curNode[0] == 'c' and c_size < 10:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     c_size += 1\n",
    "                \n",
    "#                 walk_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/het_neigh_train.txt', 'w') as file:\n",
    "#     for node in list(random_walks.keys()):\n",
    "#         file.write(node + ':' + ','.join(random_walks[node])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # node net embedding generator\n",
    "# from gensim.models import Word2Vec\n",
    "# from itertools import *\n",
    "# dimen = 128\n",
    "# window = 5\n",
    "\n",
    "# def read_random_walk_corpus():\n",
    "#     walks, node_ids = [], []\n",
    "#     with open(\"data/het_neigh_train.txt\", 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         for i, line in enumerate(lines):\n",
    "#             parts = line.strip().split(':')\n",
    "#             node_ids.append(parts[0])\n",
    "#             path = parts[1].split(',')\n",
    "#             walks.append(path)\n",
    "\n",
    "#     return walks, node_ids\n",
    "\n",
    "# walk_corpus, node_ids = read_random_walk_corpus()\n",
    "\n",
    "# # Train the Word2Vec model\n",
    "# model = Word2Vec(walk_corpus, vector_size=dimen, window=window, min_count=0, workers=8, sg=1, hs=0, negative=5)\n",
    "\n",
    "# # Save node embeddings to a text file\n",
    "# with open(\"node_net_embedding.txt\", 'w') as file:\n",
    "#     for i, node_id in enumerate(node_ids):\n",
    "#         embedding = model.wv[node_id]\n",
    "#         embedding_str = \" \".join(str(val) for val in embedding)\n",
    "#         file.write(node_id + \" \" + embedding_str + \"\\n\")\n",
    "# model.wv.save_word2vec_format(\"data/node_net_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     p_c_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m c_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39melif\u001b[39;00m node_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     t_a_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m a_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     t_p_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m p_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     t_t_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m t_edge_list])\n",
      "\u001b[1;32m/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     p_c_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m c_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39melif\u001b[39;00m node_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     t_a_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39;49mtensor([[node_id, \u001b[39mint\u001b[39;49m(node[\u001b[39m1\u001b[39;49m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m a_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     t_p_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m p_edge_list])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bananta/home/deependra/project/23-hetero-smote/HeteroG/dblp/data_gen.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     t_t_edge_index\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mtensor([[node_id, \u001b[39mint\u001b[39m(node[\u001b[39m1\u001b[39m:])]]) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m t_edge_list])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# a_edge_list = []\n",
    "# p_edge_list = []\n",
    "# t_edge_list = []\n",
    "# c_edge_list = []\n",
    "# a_a_edge_index = []\n",
    "# a_p_edge_index = []\n",
    "# a_t_edge_index = []\n",
    "# a_c_edge_index = []\n",
    "# p_a_edge_index = []\n",
    "# p_p_edge_index = []\n",
    "# p_t_edge_index = []\n",
    "# p_c_edge_index = []\n",
    "# t_a_edge_index = []\n",
    "# t_p_edge_index = []\n",
    "# t_t_edge_index = []\n",
    "# t_c_edge_index = []\n",
    "# c_a_edge_index = []\n",
    "# c_p_edge_index = []\n",
    "# c_t_edge_index = []\n",
    "# c_c_edge_index = []\n",
    "\n",
    "# with open(\"data/het_neigh_train.txt\", 'r') as file:            \n",
    "#     lines = file.readlines()\n",
    "        \n",
    "# for i, line in enumerate(lines):\n",
    "#     line = line.strip()\n",
    "#     node_type = re.split(':', line)[0][0]\n",
    "#     node_id = int(re.split(':', line)[0][1:])\n",
    "#     neigh_list = re.split(',', re.split(':', line)[1].strip())\n",
    "    \n",
    "#     a_edge_list = [node for node in neigh_list if node.startswith('a')]\n",
    "#     p_edge_list = [node for node in neigh_list if node.startswith('p')]\n",
    "#     t_edge_list = [node for node in neigh_list if node.startswith('t')]\n",
    "#     c_edge_list = [node for node in neigh_list if node.startswith('c')]\n",
    "    \n",
    "#     # Count the frequency of elements starting with 'r', 'u', and 'b'\n",
    "#     a_counts = Counter(a_edge_list)\n",
    "#     p_counts = Counter(p_edge_list)\n",
    "#     t_counts = Counter(t_edge_list)\n",
    "#     c_counts = Counter(c_edge_list)\n",
    "#     # print(a_counts)\n",
    "    \n",
    "#     a_edge_list = [node for node, count in a_counts.most_common(8)]\n",
    "#     p_edge_list = [node for node, count in p_counts.most_common(10)]\n",
    "#     t_edge_list = [node for node, count in t_counts.most_common(8)]\n",
    "#     c_edge_list = [node for node, count in c_counts.most_common(3)]\n",
    "#     # print(a_edge_list)\n",
    "    \n",
    "#     if node_type == 'a':\n",
    "#         a_a_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in a_edge_list])\n",
    "#         a_p_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in p_edge_list])\n",
    "#         a_t_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in t_edge_list])\n",
    "#         a_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "#     elif node_type == 'p':\n",
    "#         p_a_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in a_edge_list])\n",
    "#         p_p_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in p_edge_list])\n",
    "#         p_t_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in t_edge_list])\n",
    "#         p_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "#     elif node_type == 't':\n",
    "#         t_a_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in a_edge_list])\n",
    "#         t_p_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in p_edge_list])\n",
    "#         t_t_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in t_edge_list])\n",
    "#         t_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "#     else:\n",
    "#         c_a_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in a_edge_list])\n",
    "#         c_p_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in p_edge_list])\n",
    "#         c_t_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in t_edge_list])\n",
    "#         c_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "\n",
    "# # Concatenate the list of tensors into a single tensor\n",
    "# a_a_edge_index = torch.cat(a_a_edge_index, dim=0).t().contiguous()\n",
    "# a_p_edge_index = torch.cat(a_p_edge_index, dim=0).t().contiguous()\n",
    "# a_t_edge_index = torch.cat(a_t_edge_index, dim=0).t().contiguous()\n",
    "# a_c_edge_index = torch.cat(a_c_edge_index, dim=0).t().contiguous()\n",
    "# p_a_edge_index = torch.cat(p_a_edge_index, dim=0).t().contiguous()\n",
    "# p_p_edge_index = torch.cat(p_p_edge_index, dim=0).t().contiguous()\n",
    "# p_t_edge_index = torch.cat(p_t_edge_index, dim=0).t().contiguous()\n",
    "# p_c_edge_index = torch.cat(p_c_edge_index, dim=0).t().contiguous()\n",
    "# t_a_edge_index = torch.cat(t_a_edge_index, dim=0).t().contiguous()\n",
    "# t_p_edge_index = torch.cat(t_p_edge_index, dim=0).t().contiguous()\n",
    "# t_t_edge_index = torch.cat(t_t_edge_index, dim=0).t().contiguous()\n",
    "# t_c_edge_index = torch.cat(t_c_edge_index, dim=0).t().contiguous()\n",
    "# c_a_edge_index = torch.cat(c_a_edge_index, dim=0).t().contiguous()\n",
    "# c_p_edge_index = torch.cat(c_p_edge_index, dim=0).t().contiguous()\n",
    "# c_t_edge_index = torch.cat(c_t_edge_index, dim=0).t().contiguous()\n",
    "# c_c_edge_index = torch.cat(c_c_edge_index, dim=0).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_d = 128\n",
    "# A_n, P_n, T_n, C_n = 4057, 14328, 7723, 20\n",
    "# a_embed = torch.zeros(A_n, embed_d, dtype=torch.float32)\n",
    "# p_embed = torch.zeros(P_n, embed_d, dtype=torch.float32)\n",
    "# t_embed = torch.zeros(T_n, embed_d, dtype=torch.float32) \n",
    "# c_embed = torch.zeros(C_n, embed_d, dtype=torch.float32) \n",
    "\n",
    "# with open(\"data/node_net_embedding.txt\", 'r') as file:\n",
    "#     lines =  file.readlines()\n",
    "    \n",
    "# for i, line in enumerate(lines):\n",
    "#     entries = line.strip().split()\n",
    "#     node_id = int(entries[0][1:])\n",
    "#     node_type = entries[0][0]\n",
    "#     embed_list = entries[1:]\n",
    "    \n",
    "#     if node_type == 'a':\n",
    "#         a_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "#     elif node_type == 'p':\n",
    "#         p_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "#     elif node_type == 't':\n",
    "#         t_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "#     elif node_type == 'c':\n",
    "#         c_embed[node_id] =  torch.tensor([float(value) for value in embed_list], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import HeteroData\n",
    "# data2 = HeteroData()\n",
    "\n",
    "# data2['a'].num_nodes = A_n\n",
    "# data2['p'].num_nodes = P_n\n",
    "# data2['t'].num_nodes = T_n\n",
    "# data2['c'].num_nodes = C_n\n",
    "\n",
    "# data2['a_embed'].x = a_embed\n",
    "# data2['p_embed'].x = p_embed\n",
    "# data2['t_embed'].x = t_embed\n",
    "# data2['c_embed'].x = c_embed\n",
    "    \n",
    "# data2['a'].x = a_data\n",
    "# data2['p'].x = p_data\n",
    "# data2['t'].x = t_data\n",
    "\n",
    "# data2['a'].y = a_label\n",
    "\n",
    "# data2['a', 'walk', 'a'].edge_index = a_a_edge_index\n",
    "# data2['a', 'walk', 'p'].edge_index = a_p_edge_index\n",
    "# data2['a', 'walk', 't'].edge_index = a_t_edge_index\n",
    "# data2['a', 'walk', 'c'].edge_index = a_c_edge_index\n",
    "\n",
    "# data2['p', 'walk', 'a'].edge_index = p_a_edge_index\n",
    "# data2['p', 'walk', 'p'].edge_index = p_p_edge_index\n",
    "# data2['p', 'walk', 't'].edge_index = p_t_edge_index\n",
    "# data2['p', 'walk', 'c'].edge_index = p_c_edge_index\n",
    "\n",
    "# data2['t', 'walk', 'a'].edge_index = t_a_edge_index\n",
    "# data2['t', 'walk', 'p'].edge_index = t_p_edge_index\n",
    "# data2['t', 'walk', 't'].edge_index = t_t_edge_index\n",
    "# data2['t', 'walk', 'c'].edge_index = t_c_edge_index\n",
    "\n",
    "# data2['c', 'walk', 'a'].edge_index = c_a_edge_index\n",
    "# data2['c', 'walk', 'p'].edge_index = c_p_edge_index\n",
    "# data2['c', 'walk', 't'].edge_index = c_t_edge_index\n",
    "# data2['c', 'walk', 'c'].edge_index = c_c_edge_index\n",
    "\n",
    "# data2['a','to','p'].edge_index = ap_edges\n",
    "# data2['p','to','a'].edge_index = pa_edges\n",
    "# data2['p','to','t'].edge_index = pt_edges\n",
    "# data2['t','to','p'].edge_index = tp_edges\n",
    "# data2['p','to','c'].edge_index = pc_edges\n",
    "# data2['c','to','p'].edge_index = cp_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(data2, \"data/dblp_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = torch.load('data/dblp_data.pt')\n",
    "# print(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
